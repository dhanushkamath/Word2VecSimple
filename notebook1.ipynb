{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "WORD2VEC IN TENSORFLOW using the Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a slight modification of the word2vec tensorflow tutorial. This is labeled heavily so that the significance of each and every line is properly \n",
    "comprehended by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "import urllib\n",
    "import collections\n",
    "import numpy as np\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to open the text file and create a list of words. The total number of unique words are also printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 1061396),\n ('of', 593677),\n ('and', 416629),\n ('one', 411764),\n ('in', 372201),\n ('a', 325873),\n ('to', 316376),\n ('zero', 264975),\n ('nine', 250430),\n ('two', 192644)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(filename = 'text8'):\n",
    "    text_file = open('data/'+str(filename),'r')\n",
    "    words = text_file.read().split()\n",
    "    return words\n",
    "\n",
    "words = read_data()\n",
    "print(len(set(words))) # Prints the total number of unique words\n",
    "\n",
    "count = collections.Counter(words) # To get the count of each words and visualize the top 10 most frequent words\n",
    "count.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to create data,count, word_dictionary and reversed dictionary. Rare words (words \n",
    "outside our definded vocabulary) are replaced with the 'UNK' token. 'word_dictionary' is a dictionary which maps the actual word to it's integer representation. 'count' will be a list, each element being the word and its number of occurrence.(First element of count is 'UNK' and the count of all words not present in the vocabulary that we define). 'data' is a list which is obtained by substituting each \n",
    "word in 'words' with its integer representation (the input data generated from read_data). So basically, we convert words to integer representation. 'reversed_dictionary' is the reverse mapping of 'word_dictionary' (from integer labels to words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def create_train_data(words,vocab_size):\n",
    "    # initially setting the first element of 'count' as the 'UNK' token (for rare words) and its occurrence as -1\n",
    "    # Its occurence will be changed later\n",
    "    count = [['UNK',-1]] \n",
    "    # Populating the count list with the rest of the words as per vocab_size. (We take the most common words)\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size-1)) # 1 less as the first element is 'UNK'\n",
    "    \n",
    "    # Now creating 'word_dictionary' which maps words to integer labels.\n",
    "    word_dictionary = dict()\n",
    "    for word,occurence in count:\n",
    "        word_dictionary[word] = len(word_dictionary) # So it assigns an integer label as the dictionary fills up.\n",
    "    \n",
    "    # Now creating 'data' which converts words list to a list of its integer representations\n",
    "    # We only take words defined in the word_dictionary as we have a fixed vocabulary(vocab_size)\n",
    "    # All other words are 'UNK'\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in word_dictionary: \n",
    "            index = word_dictionary[word]\n",
    "        else:\n",
    "            index = 0 # Word not in dictionary, so label is 0 which is 'UNK'\n",
    "            unk_count += 1 # To get the unknown count at the end to update the first element of 'count'\n",
    "        data.append(index)\n",
    "    \n",
    "    # updating the first element of count. i.e, updating the count of 'UNK'\n",
    "    count[0][1] = unk_count \n",
    "    # To get the reverse mapping\n",
    "    reversed_dictionary = dict(zip(word_dictionary.values(),word_dictionary.keys())) \n",
    "    return data, count, word_dictionary, reversed_dictionary\n",
    "\n",
    "# Print and check the visualize these objects\n",
    "data, count, word_dictionary, reversed_dictionary = create_train_data(words,vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data prepared. Checking the first 10 elements of data and count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5244, 3081, 12, 6, 195, 2, 3137, 46, 59, 156],\n [['UNK', 418391],\n  ('the', 1061396),\n  ('of', 593677),\n  ('and', 416629),\n  ('one', 411764),\n  ('in', 372201),\n  ('a', 325873),\n  ('to', 316376),\n  ('zero', 264975),\n  ('nine', 250430)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10], count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0 \n",
    "# This is a global variable and is declared as global in the function below to keep track of the index of data so as to generate the next batch.\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  if data_index + span > len(data):\n",
    "    data_index = 0\n",
    "  buffer.extend(data[data_index:data_index + span])\n",
    "  #print(\"Buffer\")\n",
    "  #print(buffer)\n",
    "  data_index += span\n",
    "  for i in range(batch_size // num_skips): \n",
    "      # basically i acts as a counter for taking the target word. Batch size is 8. 2 context words are taken,\n",
    "      # 1 on the left and 1 on the right. So 8/2 = 4. Hence we get 4 target words. Uncomment the print statements to visualize. \n",
    "      \n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    random.shuffle(context_words)\n",
    "    words_to_use = collections.deque(context_words)\n",
    "    #print(\"Words to use\")\n",
    "    #print(words_to_use)\n",
    "    for j in range(num_skips):\n",
    "      #print(\"prev Batch\")\n",
    "      #print(batch)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]  # Target Word. \n",
    "      # i*num_skips + j basically acts as a counter for the indices of the batch, just like writing numbers like 11 as (1*10 + 1)\n",
    "      context_word = words_to_use.pop() \n",
    "      # Grab the index of a context word from words_to_use and then pop it from the deque. So now this popped index is stored in context_word\n",
    "      labels[i * num_skips + j, 0] = buffer[context_word] # Now use this popped word  as the label. \n",
    "      # (We are predicting context words from center word)\n",
    "      #print(\"Batch\")\n",
    "      #print(batch)\n",
    "      #print(\"Labels\")\n",
    "      #print(labels)\n",
    "    if data_index == len(data):\n",
    "      buffer[:] = data[:span]\n",
    "      data_index = span\n",
    "    else:\n",
    "      buffer.append(data[data_index])\n",
    "      data_index += 1\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing a batch of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3081 originated -> 12 as\n3081 originated -> 5244 anarchism\n12 as -> 6 a\n12 as -> 3081 originated\n6 a -> 195 term\n6 a -> 12 as\n195 term -> 2 of\n195 term -> 6 a\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reversed_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reversed_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulding the skip-gram model and the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label. Use 2 when skip_window = 1 (So it generates 2 pairs for one word)\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False) # generates a random sized subset in the range (o, valid_window)\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "     # setting placeholders and constants for input data, labels and validation dataset\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "     \n",
    "    # defining a variable for the word embeddings and initializing them with numbers between -1 and 1.\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size],-1.0,1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)  \n",
    "    # This can be taken as the output of a multiplication happening at the first hidden layer of the neural net. (just for understanding)\n",
    "    # Basically like a multiplication in which input tensor is a one hot. ( the hot element corresponding to the ids of words given by inputs)\n",
    "    # But internally during backprop, its like a dictionary which reverse maps (during backprop) so that it sends \n",
    "    # out the gradients only to those rows of  embeddings that were used in that particular batch. Hence it will be a much more faster than\n",
    "    # representing this operation with a typical neural network layer with the embeddings as its weights.\n",
    "    # This picks up all the tensors in the embeddings according to the ids obtained from train_inputs\n",
    "    # Remember that train_inputs is just a tensor of a set of integers governed by batch_size which are \n",
    "    # the integer representations of the words in data \n",
    "     \n",
    "    # Constructing weights and biases for NCE Loss\n",
    "    nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev = 1.0/math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    # in NCE loss, 'inputs' actually represent the input to the nce_loss and not the input of the neural net.\n",
    "    # The input to the nce_loss will be the output of the previous layer. The previous layer is the embedding_lookup layer\n",
    "    #\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "    \n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "          normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "          valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high level explanation for the optimization of nce loss obtained from stackoverflow (given by curator23) : \n",
    "\"The embeddings Tensor is your final output matrix. It maps words to vectors. Use this in your word prediction graph.\n",
    "The input matrix is a batch of centre-word : context-word pairs (train_input and train_label respectively) generated from the training text.\n",
    "While the exact workings of the nce_loss op are not yet know to me, the basic idea is that it uses a single layer network (parameters nce_weights and nce_biases) to map an input vector (selected from embeddings using the embed op) to an output word, and then compares the output to the training label (an adjacent word in the training text) and also to a random sub-sample (num_sampled) of all other words in the vocab, and then modifies the input vector (stored in embeddings) and the network parameters to minimise the error.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\nAverage loss at step  0 :  282.584777832\nNearest to nine: subsist, anisotropic, fellows, findable, freeciv, rumors, seventeenth, asn,\nNearest to UNK: acrimonious, oxen, qing, mkultra, literal, mutinied, meddle, mercenary,\nNearest to four: lifespan, advantages, dhimmi, eudoxia, tert, suzy, straightforwardly, ramesses,\nNearest to one: iucn, gamble, rf, elves, barton, anz, vitry, gilt,\nNearest to he: registration, local, paraphrased, rabinowitz, supers, gourmet, anatoly, endocrinology,\nNearest to by: prom, ehud, southern, assaulted, sectarianism, indochina, plunder, pledges,\nNearest to his: boo, radiosity, opus, jungles, unit, franconia, oceanography, color,\nNearest to no: uncontrolled, specially, sn, mouthpieces, plugs, foix, miles, lineage,\nNearest to its: crashed, sophistication, appearances, unicellular, analysing, gurion, humorous, chloride,\nNearest to often: crevasses, prosthetic, pompidou, outstripped, reconquest, tapes, lifespan, divided,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest to th: weld, brunel, martins, point, dioxide, cezanne, observing, inwardly,\nNearest to their: anacreon, af, appendices, clip, marshalling, scenery, sharper, minarchists,\nNearest to been: adopts, immune, stark, christensen, peyton, datum, heterocyclic, soul,\nNearest to it: falls, drucker, runs, furor, sciences, louisiana, drivers, grotesque,\nNearest to there: nowell, guernica, withered, adolphus, triggering, sara, warns, lutheranism,\nNearest to when: several, disparagingly, sandro, serial, communism, calendars, warned, lacquer,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  2000 :  113.735357479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  4000 :  52.778852736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  6000 :  33.5482197268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  8000 :  23.7463875093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  10000 :  17.3574590499\nNearest to nine: eight, zero, vs, var, seven, one, phi, six,\nNearest to UNK: mosque, yeast, one, vs, bang, and, the, gb,\nNearest to four: eight, zero, var, nine, one, three, rudolph, two,\nNearest to one: two, var, nine, gb, zero, UNK, seven, mosque,\nNearest to he: it, they, anatoly, pseudopods, vocals, local, vs, measure,\nNearest to by: and, in, of, is, wire, confident, var, as,\nNearest to his: the, circumcision, its, del, damage, theological, settings, edmonton,\nNearest to no: to, specially, deism, revolutionaries, servant, socialism, proposed, miles,\nNearest to its: the, appearances, his, circumcision, training, vienna, charge, typical,\nNearest to often: divided, lifespan, authors, stylized, manifestations, tapes, their, biography,\nNearest to th: point, observing, six, zero, three, var, rotate, born,\nNearest to their: the, clip, style, concentrated, remain, often, survey, vs,\nNearest to been: immune, adopts, soul, paradigms, vs, rays, honour, stark,\nNearest to it: falls, vocals, vs, vaccine, he, and, lieutenant, wimbledon,\nNearest to there: vs, wimbledon, geography, presidential, bckgr, gram, selden, clockwork,\nNearest to when: mosque, several, yeast, vs, zero, communism, legal, serial,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  12000 :  14.0689575607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  14000 :  11.9180101365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  16000 :  10.004420069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  18000 :  8.4035275017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  20000 :  8.02782380855\nNearest to nine: eight, six, seven, zero, five, four, vs, operatorname,\nNearest to UNK: dasyprocta, circ, operatorname, agouti, yeast, mosque, two, venus,\nNearest to four: eight, zero, nine, three, two, five, six, seven,\nNearest to one: two, dasyprocta, four, agouti, three, eight, seven, operatorname,\nNearest to he: it, they, and, she, argues, who, dasyprocta, vs,\nNearest to by: was, in, and, is, with, as, for, from,\nNearest to his: the, its, her, their, s, del, circumcision, opus,\nNearest to no: specially, unequal, to, servant, revolutionaries, socialism, and, soprano,\nNearest to its: his, the, their, crashed, appearances, training, circumcision, typical,\nNearest to often: divided, tapes, manifestations, stylized, lifespan, authors, some, audible,\nNearest to th: three, six, zero, four, eight, one, observing, point,\nNearest to their: the, his, its, anacreon, operatorname, remain, her, clip,\nNearest to been: adopts, immune, by, was, who, have, soul, paradigms,\nNearest to it: he, this, there, they, falls, vocals, apatosaurus, vs,\nNearest to there: it, wimbledon, gram, geography, vs, he, which, paint,\nNearest to when: mosque, several, yeast, legal, was, vs, and, catholics,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  22000 :  7.03642044675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  24000 :  6.85834533679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  26000 :  6.72017207646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  28000 :  6.34312397707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  30000 :  5.97918035579\nNearest to nine: eight, seven, six, five, four, zero, three, operatorname,\nNearest to UNK: dasyprocta, circ, operatorname, mosque, agouti, three, aediles, arin,\nNearest to four: five, eight, seven, six, three, nine, two, zero,\nNearest to one: two, four, three, seven, eight, dasyprocta, agouti, operatorname,\nNearest to he: it, they, she, who, there, argues, zero, vs,\nNearest to by: in, with, was, from, and, as, is, be,\nNearest to his: her, their, the, its, s, del, circumcision, opus,\nNearest to no: sponsors, specially, unequal, to, revolutionaries, servant, a, nine,\nNearest to its: the, his, their, crashed, a, analysing, appearances, training,\nNearest to often: divided, tapes, also, audible, lifespan, manifestations, authors, stylized,\nNearest to th: six, three, eight, one, two, four, seven, point,\nNearest to their: the, his, its, her, remain, operatorname, lithuanian, a,\nNearest to been: by, adopts, sponsors, was, be, immune, who, have,\nNearest to it: he, this, there, they, which, agouti, vs, sponsors,\nNearest to there: it, he, which, sponsors, vs, cytoplasm, amalthea, wimbledon,\nNearest to when: mosque, yeast, and, several, lemon, was, legal, five,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  32000 :  5.9756525476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  34000 :  5.69859705305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  36000 :  5.75849120533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  38000 :  5.55741627693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  40000 :  5.24219259167\nNearest to nine: eight, seven, zero, six, five, four, three, operatorname,\nNearest to UNK: dasyprocta, operatorname, circ, agouti, recitative, arin, four, aediles,\nNearest to four: five, three, six, eight, seven, zero, two, one,\nNearest to one: two, four, three, eight, six, five, seven, dasyprocta,\nNearest to he: it, they, she, who, there, zero, we, vs,\nNearest to by: was, with, in, be, is, as, been, from,\nNearest to his: their, her, the, its, s, circumcision, del, conspired,\nNearest to no: sponsors, unequal, a, specially, it, revolutionaries, still, nine,\nNearest to its: their, the, his, a, crashed, analysing, training, appearances,\nNearest to often: divided, also, tapes, authors, chaotic, lifespan, audible, ras,\nNearest to th: six, eight, seven, three, accumulator, zero, four, one,\nNearest to their: the, its, his, her, recitative, beadwork, operatorname, agouti,\nNearest to been: be, was, by, sponsors, adopts, reliance, were, who,\nNearest to it: he, this, there, they, which, agouti, that, not,\nNearest to there: it, which, he, they, sponsors, now, cytoplasm, amalthea,\nNearest to when: mosque, lemon, was, and, since, yeast, recitative, that,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  42000 :  5.34693013203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  44000 :  5.24501333344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  46000 :  5.23082812393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  48000 :  5.22957700038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  50000 :  4.98618395913\nNearest to nine: eight, six, seven, zero, five, three, four, operatorname,\nNearest to UNK: dasyprocta, kapoor, agouti, circ, four, recitative, marek, operatorname,\nNearest to four: three, six, five, eight, two, seven, one, kapoor,\nNearest to one: two, four, three, six, eight, seven, five, kapoor,\nNearest to he: it, they, she, who, there, we, truetype, this,\nNearest to by: was, be, with, from, in, as, is, and,\nNearest to his: their, her, the, its, s, recitative, agouti, circumcision,\nNearest to no: sponsors, unequal, it, a, still, or, only, and,\nNearest to its: their, his, the, analysing, crashed, a, her, homomorphism,\nNearest to often: also, divided, now, sometimes, tapes, chaotic, ras, audible,\nNearest to th: six, three, eight, seven, four, accumulator, one, five,\nNearest to their: his, its, the, her, kapoor, some, marek, recitative,\nNearest to been: be, was, were, sponsors, by, adopts, reliance, are,\nNearest to it: he, this, there, they, which, kapoor, agouti, dasyprocta,\nNearest to there: it, which, they, he, sponsors, cytoplasm, now, amalthea,\nNearest to when: since, after, and, mosque, four, recitative, three, was,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52000 :  5.05709727466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  54000 :  5.17568934417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  56000 :  5.07141618419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  58000 :  5.04957467067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  60000 :  4.94871200526\nNearest to nine: eight, seven, six, four, five, zero, ursus, operatorname,\nNearest to UNK: dasyprocta, kapoor, ursus, circ, pulau, agouti, operatorname, thibetanus,\nNearest to four: five, six, three, eight, seven, two, kapoor, nine,\nNearest to one: two, six, four, three, five, dasyprocta, eight, kapoor,\nNearest to he: it, they, she, who, there, we, but, this,\nNearest to by: was, be, with, as, from, four, five, been,\nNearest to his: their, her, its, the, s, my, pulau, recitative,\nNearest to no: unequal, sponsors, a, it, only, westphalia, franc, positions,\nNearest to its: their, his, the, her, analysing, recitative, a, homomorphism,\nNearest to often: pulau, also, divided, sometimes, now, it, ras, tapes,\nNearest to th: six, four, seven, eight, three, accumulator, five, dioxide,\nNearest to their: its, his, the, her, recitative, kapoor, pulau, some,\nNearest to been: be, was, were, by, reliance, sponsors, adopts, are,\nNearest to it: he, this, there, which, they, agouti, she, kapoor,\nNearest to there: it, they, which, he, sponsors, cytoplasm, now, lauter,\nNearest to when: after, since, five, was, four, mosque, recitative, six,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  62000 :  5.0174506073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  64000 :  4.82829598403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  66000 :  4.59480603361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  68000 :  4.96854473853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  70000 :  4.86998276556\nNearest to nine: eight, seven, six, five, zero, four, ursus, operatorname,\nNearest to UNK: dasyprocta, kapoor, ursus, mico, pulau, circ, callithrix, operatorname,\nNearest to four: six, five, three, eight, seven, two, nine, kapoor,\nNearest to one: six, two, four, three, five, dasyprocta, seven, kapoor,\nNearest to he: it, she, they, who, there, we, never, remnant,\nNearest to by: was, be, with, as, seizures, from, kapoor, in,\nNearest to his: their, her, its, the, my, thaler, s, pulau,\nNearest to no: unequal, sponsors, it, only, a, westphalia, positions, franc,\nNearest to its: their, his, the, her, thaler, analysing, homomorphism, recitative,\nNearest to often: pulau, also, now, sometimes, divided, commonly, farewell, simply,\nNearest to th: six, three, seven, four, eight, accumulator, one, dioxide,\nNearest to their: its, his, the, her, some, recitative, kapoor, pulau,\nNearest to been: be, was, were, reliance, by, are, sponsors, adopts,\nNearest to it: he, this, there, which, they, she, not, callithrix,\nNearest to there: it, they, which, he, now, sponsors, still, this,\nNearest to when: after, since, before, was, however, if, where, mosque,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  72000 :  4.74126870692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  74000 :  4.7994087069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  76000 :  4.71473241282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  78000 :  4.80887523252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  80000 :  4.81248695421\nNearest to nine: eight, six, seven, five, four, zero, ursus, operatorname,\nNearest to UNK: dasyprocta, ursus, kapoor, agouti, callithrix, mico, circ, operatorname,\nNearest to four: five, six, three, seven, eight, two, nine, zero,\nNearest to one: six, seven, two, five, kapoor, dasyprocta, three, michelob,\nNearest to he: it, they, she, who, there, we, never, you,\nNearest to by: was, be, as, with, from, operatorname, in, kapoor,\nNearest to his: their, her, its, the, s, my, thaler, polyn,\nNearest to no: unequal, sponsors, it, only, still, westphalia, franc, positions,\nNearest to its: their, his, the, her, thaler, analysing, jethro, a,\nNearest to often: pulau, also, sometimes, now, commonly, divided, still, usually,\nNearest to th: six, seven, eight, four, accumulator, three, dioxide, five,\nNearest to their: its, his, her, the, thaler, pulau, kapoor, recitative,\nNearest to been: be, was, were, reliance, by, sponsors, had, are,\nNearest to it: he, this, there, which, they, she, callithrix, not,\nNearest to there: it, they, he, which, now, still, this, cytoplasm,\nNearest to when: after, since, however, before, if, where, busan, recitative,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  82000 :  4.77211881125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  84000 :  4.76178852105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  86000 :  4.78316018534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  88000 :  4.74671554017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  90000 :  4.73551342165\nNearest to nine: eight, seven, six, five, zero, four, ursus, three,\nNearest to UNK: dasyprocta, ursus, mico, kapoor, circ, pulau, busan, operatorname,\nNearest to four: three, five, seven, six, eight, two, one, kapoor,\nNearest to one: four, two, three, seven, six, five, eight, kapoor,\nNearest to he: it, she, they, who, there, we, never, but,\nNearest to by: was, be, as, with, in, from, busan, kapoor,\nNearest to his: their, her, its, the, my, s, thaler, transistor,\nNearest to no: unequal, sponsors, it, only, a, still, any, westphalia,\nNearest to its: their, his, the, her, thaler, analysing, recitative, jethro,\nNearest to often: sometimes, also, pulau, now, commonly, usually, still, generally,\nNearest to th: six, seven, eight, nine, accumulator, four, three, five,\nNearest to their: its, his, her, the, recitative, some, agouti, pulau,\nNearest to been: be, was, were, reliance, by, are, had, sponsors,\nNearest to it: he, this, there, she, they, which, but, not,\nNearest to there: it, they, he, which, now, still, this, cytoplasm,\nNearest to when: after, if, before, since, however, where, while, busan,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  92000 :  4.67600551403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  94000 :  4.71477692652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  96000 :  4.69089664531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  98000 :  4.60148849851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  100000 :  4.69934381998\nNearest to nine: eight, seven, six, five, zero, four, three, ursus,\nNearest to UNK: kapoor, dasyprocta, ursus, mico, callithrix, operatorname, circ, peacocks,\nNearest to four: five, three, seven, eight, six, two, zero, kapoor,\nNearest to one: four, seven, two, five, six, three, kapoor, eight,\nNearest to he: it, she, they, who, there, we, never, but,\nNearest to by: be, was, as, operatorname, seizures, kapoor, busan, seven,\nNearest to his: their, her, its, the, my, s, thaler, our,\nNearest to no: it, unequal, sponsors, only, any, nine, franc, positions,\nNearest to its: their, his, the, her, thaler, jethro, analysing, some,\nNearest to often: sometimes, also, pulau, commonly, now, usually, generally, still,\nNearest to th: six, seven, accumulator, nine, eight, dioxide, three, brunel,\nNearest to their: its, his, her, the, some, pulau, agouti, thaler,\nNearest to been: be, was, were, reliance, become, by, had, sponsors,\nNearest to it: he, there, this, she, they, which, agouti, but,\nNearest to there: it, they, he, which, now, still, this, sponsors,\nNearest to when: after, if, before, since, while, where, however, busan,\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print('Initialized')\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print('Average loss at step ', step, ': ', average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reversed_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = 'Nearest to %s:' % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reversed_dictionary[nearest[k]]\n",
    "          log_str = '%s %s,' % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00751682, -0.05371699, -0.07132055, ..., -0.13933519,\n        -0.02045279, -0.04116968],\n       [ 0.08291626,  0.11531038, -0.08103251, ...,  0.12472363,\n         0.05915395, -0.06485777],\n       [ 0.12645645,  0.12861994, -0.15893146, ...,  0.02102033,\n        -0.08008344, -0.17346135],\n       ..., \n       [ 0.07011682,  0.00313191, -0.05450524, ..., -0.07874514,\n        -0.05461983, -0.1422379 ],\n       [ 0.03490714, -0.02096882,  0.01477128, ..., -0.02078016,\n        -0.04375534, -0.05463719],\n       [ 0.06420515, -0.06192603, -0.09500519, ..., -0.0512049 ,\n        -0.07853603,  0.0226666 ]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}