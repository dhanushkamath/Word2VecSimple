{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD2VEC IN TENSORFLOW"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 5,
   "source": [
    "This is a slight modification of the word2vec tensorflow tutorial. This is labeled heavily so that the significance of each and every line is properly \n",
    "comprehended by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "import urllib\n",
    "import collections\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to open the text file and create a list of words. The total number of unique words are also printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 1061396),\n ('of', 593677),\n ('and', 416629),\n ('one', 411764),\n ('in', 372201),\n ('a', 325873),\n ('to', 316376),\n ('zero', 264975),\n ('nine', 250430),\n ('two', 192644)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_data(filename = 'text8'):\n",
    "    text_file = open('data/'+str(filename),'r')\n",
    "    words = text_file.read().split()\n",
    "    return words\n",
    "\n",
    "words = read_data()\n",
    "print(len(set(words))) # Prints the total number of unique words\n",
    "\n",
    "count = collections.Counter(words) # To get the count of each words and visualize the top 10 most frequent words\n",
    "count.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to create data,count, word_dictionary and reversed dictionary. Rare words (words \n",
    "outside our definded vocabulary) are replaced with the 'UNK' token. 'word_dictionary' is a dictionary which maps the actual word to it's integer representation. 'count' will be a list, each element being the word and its number of occurrence.(First element of count is 'UNK' and the count of all words not present in the vocabulary that we define). 'data' is a list which is obtained by substituting each \n",
    "word in 'words' with its integer representation (the input data generated from read_data). So basically, we convert words to integer representation. 'reversed_dictionary' is the reverse mapping of 'word_dictionary' (from integer labels to words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data(words,vocab_size):\n",
    "    # initially setting the first element of 'count' as the 'UNK' token (for rare words) and its occurrence as -1\n",
    "    # Its occurence will be changed later\n",
    "    count = [['UNK',-1]] \n",
    "    # Populating the count list with the rest of the words as per vocab_size. (We take the most common words)\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size-1)) # 1 less as the first element is 'UNK'\n",
    "    \n",
    "    # Now creating 'word_dictionary' which maps words to integer labels.\n",
    "    word_dictionary = dict()\n",
    "    for word,occurence in count:\n",
    "        word_dictionary[word] = len(word_dictionary) # So it assigns an integer label as the dictionary fills up.\n",
    "    \n",
    "    # Now creating 'data' which converts words list to a list of its integer representations\n",
    "    # We only take words defined in the word_dictionary as we have a fixed vocabulary(vocab_size)\n",
    "    # All other words are 'UNK'\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in word_dictionary: \n",
    "            index = word_dictionary[word]\n",
    "        else:\n",
    "            index = 0 # Word not in dictionary, so label is 0 which is 'UNK'\n",
    "            unk_count += 1 # To get the unknown count at the end to update the first element of 'count'\n",
    "        data.append(index)\n",
    "    \n",
    "    # updating the first element of count. i.e, updating the count of 'UNK'\n",
    "    count[0][1] = unk_count \n",
    "    # To get the reverse mapping\n",
    "    reversed_dictionary = dict(zip(word_dictionary.values(),word_dictionary.keys())) \n",
    "    return data, count, word_dictionary, reversed_dictionary\n",
    "\n",
    "# Print and check the visualize these objects\n",
    "data, count, word_dictionary, reversed_dictionary = create_train_data(words,50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our data prepared. Checking the first 10 elements of data and count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5244, 3081, 12, 6, 195, 2, 3137, 46, 59, 156],\n [['UNK', 418391],\n  ('the', 1061396),\n  ('of', 593677),\n  ('and', 416629),\n  ('one', 411764),\n  ('in', 372201),\n  ('a', 325873),\n  ('to', 316376),\n  ('zero', 264975),\n  ('nine', 250430)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10], count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0 \n",
    "# This is a global variable and is declared as global in the function below to keep track of the index of data so as to generate the next batch.\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  if data_index + span > len(data):\n",
    "    data_index = 0\n",
    "  buffer.extend(data[data_index:data_index + span])\n",
    "  #print(\"Buffer\")\n",
    "  #print(buffer)\n",
    "  data_index += span\n",
    "  for i in range(batch_size // num_skips): \n",
    "      # basically i acts as a counter for taking the target word. Batch size is 8. 2 context words are taken,\n",
    "      # 1 on the left and 1 on the right. So 8/2 = 4. Hence we get 4 target words. Uncomment the print statements to visualize. \n",
    "      \n",
    "    context_words = [w for w in range(span) if w != skip_window]\n",
    "    random.shuffle(context_words)\n",
    "    words_to_use = collections.deque(context_words)\n",
    "    #print(\"Words to use\")\n",
    "    #print(words_to_use)\n",
    "    for j in range(num_skips):\n",
    "      #print(\"prev Batch\")\n",
    "      #print(batch)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]  # Target Word. \n",
    "      # i*num_skips + j basically acts as a counter for the indices of the batch, just like writing numbers like 11 as (1*10 + 1)\n",
    "      context_word = words_to_use.pop() \n",
    "      # Grab the index of a context word from words_to_use and then pop it from the deque. So now this popped index is stored in context_word\n",
    "      labels[i * num_skips + j, 0] = buffer[context_word] # Now use this popped word  as the label. \n",
    "      # (We are predicting context words from center word)\n",
    "      #print(\"Batch\")\n",
    "      #print(batch)\n",
    "      #print(\"Labels\")\n",
    "      #print(labels)\n",
    "    if data_index == len(data):\n",
    "      buffer[:] = data[:span]\n",
    "      data_index = span\n",
    "    else:\n",
    "      buffer.append(data[data_index])\n",
    "      data_index += 1\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the current batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2]\ndeque([0])\n"
     ]
    }
   ],
   "source": [
    "context_words = [w for w in range(3) if w != 1]\n",
    "print(context_words)\n",
    "words_to_use = collections.deque(context_words)\n",
    "words_to_use.pop()\n",
    "print(words_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}